{"nbformat":4,"nbformat_minor":0,"metadata":{"@webio":{"lastCommId":null,"lastKernelId":null},"colab":{"name":"Lab10_questions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZQ-tHjsZ7uCa"},"source":["# Lab 10 Classification - Questions"]},{"cell_type":"markdown","metadata":{"id":"BxwqUQoIAUQX"},"source":["$\\color{red}{\\text{RULES:}}$\n","\n","1) Work individually! \n","\n","2) You can use the lecture notes (that are posted on D2L), and other materials (e.g. books, web searches).\n"]},{"cell_type":"markdown","metadata":{"id":"miOhrnrV7uCc"},"source":["## Question 1"]},{"cell_type":"markdown","metadata":{"id":"RUrii2rZAUQa"},"source":["Consider the task of predicting automobile fuel efficiency, which is characterized by miles per gallon (mpg) metric. The data file \"auto_mpg.csv\" contains the instances for this task.\n","- (1.a) Read the csv file, and store it as a pandas dataframe named \"df_mpg\"\n","- (1.b) Clean the data, convert categorical features to numerical ones and handle the missing values by replacing them with the column/feature averages (i.e. mode). \n","\n","> Hint1: In loan prediction task, for the 'LoanAmount' feature, we used **df_lp['LoanAmount'].fillna(df_lp['LoanAmount'].mode()[0], inplace=True)** to replace missing values. \n","\n","> Hint2: In loan prediction task, for the 'Gender' feature, we used **df_lp['Gender'] =df_lp['Gender'].astype('category').cat.codes** to convert categorical feature to numerical one.\n","\n","- (1.c) Add a new column to the dataframe, named mpg_binary, which is obtained based on the column \"mpg\" as follows: all the cars with a mpg value lower than 23 has a bad mpg (i.e. class label 0), and all other cars have a good mpg (i.e. class label 1). \n","- (1.d) separate the class labels (both mpg and mpg_binary), store mpg_binary as the labels and ignore mpg column all together, divide the data into 80/20 train-test split, and write the evaluation function.\n","- (1.e) Now that we have a binary classification task, perform logistic regression, knn classification, decision tree classification and random forest classification tasks. Report the performances of each model (e.g. accuracy, precision, recall) over the test set."]},{"cell_type":"markdown","metadata":{"id":"G3uQXgE8AUQc"},"source":["### Libraries"]},{"cell_type":"code","metadata":{"id":"Ca-X_LdVAUQd"},"source":["import pandas as pd\n","import numpy as np\n","import math\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","\n","from sklearn.tree import DecisionTreeClassifier\n","\n","givenDec = lambda gdVal: float('%.1f' % gdVal) # 1 digit"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"odmX0cDVAUQe"},"source":["### Solution 1.a"]},{"cell_type":"code","metadata":{"id":"nblg0uCMAUQf"},"source":["# read csv\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E6A3Y8d0AUQf"},"source":["### Solution 1.b"]},{"cell_type":"code","metadata":{"id":"ffXJ4y8bAUQg"},"source":["# convert categorical to numerical\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"bveU6u_qAUQg"},"source":["# missing values\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l69hKDu8AUQh"},"source":["### Solution 1.c"]},{"cell_type":"code","metadata":{"id":"92W8buX5AUQh"},"source":["#add a new column to the dataframe 'mpg_binary\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wstw1AUcAUQi"},"source":["### Solution 1.d"]},{"cell_type":"code","metadata":{"id":"WNGknV-WBmN2"},"source":["# separate the class label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyasCvvlBnwB"},"source":["# get train/test\n","# setting the random state to 42 which means the results will be the same each time I run the split for reproducible results."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOIvR_6qAUQj"},"source":["def evaluate_results(gTestLabel, gTestPredictions):\n","    # your code here\n","    return "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OD-3ZGhaBGkl"},"source":["### Solution 1.e"]},{"cell_type":"markdown","metadata":{"id":"gdATaEeWAUQq"},"source":["#### Logistic Regression"]},{"cell_type":"code","metadata":{"id":"uFz_in3nAUQq"},"source":["clf = LogisticRegression()\n","# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyZiio4NAUQr"},"source":["#### kNN"]},{"cell_type":"code","metadata":{"id":"98UEJi4TAUQr"},"source":["clf = KNeighborsClassifier(n_neighbors=4)\n","# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wHajmzfaAUQr"},"source":["#### Decision Tree Classification DT"]},{"cell_type":"code","metadata":{"id":"XIGYHvk6AUQs"},"source":["clf = DecisionTreeClassifier()\n","# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwBg8mHxAUQs"},"source":["#### Random Forest Classifier RF"]},{"cell_type":"code","metadata":{"id":"yyMaYuNZAUQs"},"source":["clf = RandomForestClassifier()\n","# your code here"],"execution_count":null,"outputs":[]}]}